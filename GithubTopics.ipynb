{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyOVNaZNu3MDpu2RJUtPYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridamju23/github-topics/blob/main/GithubTopics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project outline**\n",
        "\n",
        "\n",
        "*   we are going to scrape \"https://github.com/topics\"\n",
        "*   we will get a list of topics. for each topic we will get topic name,page url and description\n",
        "*   for each topic we will get top 25 repository from topic name\n",
        "*   for each repository we will get repo name,username,repo url\n",
        "*   for each topic we will create a csv file in below format\n",
        "            repo name,user name,stars,repo url\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIsTqwOEGIg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#required libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import os\n"
      ],
      "metadata": {
        "id": "rE0dvoHwhoQh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseUrl = \"https://github.com\" # address of home page,that is our base url\n",
        "# this class contains some methods to perform different small independent task\n",
        "class GithubTopics:\n",
        "  #configure of objects\n",
        "  def __init__(self,limit:int)->None:\n",
        "    self.limit =limit\n",
        "\n",
        "  def getMainSoup(self,redirectTo):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    - redirectTo (str): The URL or path to redirect to for fetching the content.\n",
        "\n",
        "    Returns:\n",
        "    BeautifulSoup: A BeautifulSoup object representing the parsed HTML content.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the HTTP request to fetch the content fails or returns a non-200 status code.\n",
        "\n",
        "    This method sends a GET request to the specified URL (baseUrl + redirectTo),\n",
        "    and upon a successful response, it creates a BeautifulSoup object to parse\n",
        "    the HTML content. If the HTTP request fails or the status code is not 200,\n",
        "    an exception is raised with an informative error message.\n",
        "    \"\"\"\n",
        "\n",
        "    topics_response = requests.get(baseUrl+redirectTo)\n",
        "    if topics_response.status_code!=200:\n",
        "      raise Exception(f\"Failed to response from {baseUrl}\")\n",
        "    return BeautifulSoup(topics_response.text)\n",
        "\n",
        "  def getTopics(self,soup):\n",
        "    \"\"\"\n",
        "    Extract a list of topics from a BeautifulSoup object representing a parsed HTML page.\n",
        "\n",
        "    Parameters:\n",
        "    - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of topic names extracted from the HTML page.\n",
        "\n",
        "    This method finds and extracts topics from the HTML page using the specified CSS class\n",
        "    ('f3 lh-condensed mb-0 mt-1 Link--primary') applied to 'p' tags. It limits the number\n",
        "    of topics returned based on the 'limit' attribute of the class instance. The extracted\n",
        "    topic names are then returned as a list.\n",
        "    \"\"\"\n",
        "    topics_class=\"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
        "    topics_tags = soup.findAll('p',{'class':topics_class},limit = self.limit)\n",
        "    topics_name =[topic.get_text() for topic in topics_tags]\n",
        "    return topics_name\n",
        "\n",
        "  def getTopicDescriptions(self,soup):\n",
        "    \"\"\"\n",
        "    Extract a list of descriptions from a BeautifulSoup object representing a parsed HTML page.\n",
        "\n",
        "    Parameters:\n",
        "    - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of descriptions extracted from the HTML page.\n",
        "\n",
        "    This method finds and extracts descriptions from the HTML page using the specified CSS class\n",
        "    (\"f5 color-fg-muted mb-0 mt-1\") applied to 'p' tags. It limits the number\n",
        "    of descriptions returned based on the 'limit' attribute of the class instance. The extracted\n",
        "    descriptions are then returned as a list.\n",
        "    \"\"\"\n",
        "    description_class =\"f5 color-fg-muted mb-0 mt-1\"\n",
        "    description_tags = soup.findAll('p',{'class':description_class},limit=self.limit)\n",
        "    descriptions =[des.text.strip() for des in description_tags]\n",
        "    return descriptions\n",
        "\n",
        "  def getTopicLinks(self,soup):\n",
        "\n",
        "    \"\"\"\n",
        "    Extract a list of links from a BeautifulSoup object representing a parsed HTML page.\n",
        "\n",
        "    Parameters:\n",
        "    - soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of links extracted from the HTML page.\n",
        "\n",
        "    This method finds and extracts links from the HTML page using the specified CSS class\n",
        "    (\"no-underline flex-grow-0\") applied to 'p' tags. It limits the number\n",
        "    of links returned based on the 'limit' attribute of the class instance. The extracted\n",
        "    links are then returned as a list.\n",
        "    \"\"\"\n",
        "    topics_ref_tags = soup.find_all('a',href=GithubTopics.topics_links,class_=\"no-underline flex-grow-0\",limit=self.limit)\n",
        "    topics_ref =[baseUrl+a_links['href'] for a_links in topics_ref_tags]\n",
        "    return topics_ref\n",
        "\n",
        "  def getDataFrame(self,topics_name,descriptions,topics_ref):\n",
        "    \"\"\"\n",
        "    Create a Pandas DataFrame from lists of topic names, descriptions, and reference links.\n",
        "\n",
        "    Parameters:\n",
        "    - topics_name (list): List of topic names.\n",
        "    - descriptions (list): List of topic descriptions.\n",
        "    - topics_ref (list): List of topic reference links.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A Pandas DataFrame containing the provided data.\n",
        "\n",
        "    This method takes lists of topic names, descriptions, and reference links and\n",
        "    creates a Pandas DataFrame with columns 'Topics', 'Descriptions', and 'Links'.\n",
        "    The DataFrame is then returned.\n",
        "    \"\"\"\n",
        "    data ={'Topics':topics_name,'Descriptions':descriptions,'Links':topics_ref}\n",
        "    github_df = pd.DataFrame(data)\n",
        "    return github_df\n",
        "\n",
        "  def to_csv(self,dataFrame,file_name):\n",
        "    \"\"\"\n",
        "    Save a Pandas DataFrame to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - dataFrame (pd.DataFrame): The Pandas DataFrame to be saved.\n",
        "\n",
        "    Prints:\n",
        "    - str: Status message indicating whether the CSV file was saved successfully or if there was an error.\n",
        "\n",
        "    This method attempts to save the provided DataFrame to a CSV file with the filename 'topics.csv'\n",
        "    in the current working directory. It prints a status message indicating the success or failure of the operation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      dataFrame.to_csv(file_name,index=False)\n",
        "      print(\"CSV file is saved successfully\")\n",
        "    except Exception as e:\n",
        "      print(f\"There is an error: {str(e)}\")\n",
        "\n",
        "  @staticmethod\n",
        "  def topics_links(href):\n",
        "    \"\"\"\n",
        "    this static method checks if href of links contains \"/topics/\" to get different topic's reference\n",
        "    \"\"\"\n",
        "    return href and re.search('/topics/',href)\n"
      ],
      "metadata": {
        "id": "D-ehdpNjjHRb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topicDriver():\n",
        "    \"\"\"\n",
        "    Driver function to scrape GitHub topics, descriptions, links, create a DataFrame, and save it to a CSV file.\n",
        "    This function initializes a GithubTopicMain instance, fetches the main soup,\n",
        "    extracts topic information, creates a DataFrame, and saves it to a CSV file.\n",
        "\n",
        "    Note: 'limit' parameter in the GithubTopicMain instantiation\n",
        "    based on specific requirements.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        limit = int(input(\"Enter the number of topics wanna get :\"))\n",
        "        gtm = GithubTopics(limit=limit)\n",
        "        redirectTo = \"/topics\"\n",
        "        soup = gtm.getMainSoup(redirectTo)\n",
        "        topics_name = gtm.getTopics(soup)\n",
        "        descriptions = gtm.getTopicDescriptions(soup)\n",
        "        topics_ref = gtm.getTopicLinks(soup)\n",
        "        df = gtm.getDataFrame(topics_name, descriptions, topics_ref)\n",
        "        file_name = redirectTo[1:] + \".csv\"\n",
        "        gtm.to_csv(df, file_name)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topicDriver()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33OyMk5eTeZP",
        "outputId": "86298a1b-f0da-4026-8cd7-c262f4b5a66b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of topics wanna get :10\n",
            "CSV file is saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GithubRepos:\n",
        "  def __init__(self,limit) -> None:\n",
        "    self.limit = limit\n",
        "\n",
        "  def getSoup(self,link):\n",
        "    \"\"\"\n",
        "    Retrieve and parse the HTML content of a each topic web page get from our earlier links.\n",
        "\n",
        "    Parameters:\n",
        "    - link (str): The URL or path to the each topic page.\n",
        "\n",
        "    Returns:\n",
        "    BeautifulSoup: A BeautifulSoup object representing the parsed HTML content.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the HTTP request to fetch the content fails or returns a non-200 status code.\n",
        "\n",
        "    This method sends a GET request to the specified URL, and upon a successful response,\n",
        "    it creates a BeautifulSoup object to parse the HTML content. If the HTTP request fails\n",
        "    or the status code is not 200, an exception is raised with an informative error message.\n",
        "    \"\"\"\n",
        "    response = requests.get(link)\n",
        "    if response.status_code!=200:\n",
        "      raise Exception(\"Failed!\")\n",
        "    return BeautifulSoup(response.text)\n",
        "\n",
        "  def getRepoInfo(self,topic_soup):\n",
        "    \"\"\"\n",
        "    Extract repository information from a BeautifulSoup object representing a topic page.\n",
        "\n",
        "    Parameters:\n",
        "    - topic_soup (BeautifulSoup): The BeautifulSoup object containing the parsed HTML content of a topic page.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing lists of user names, repo names, repo links, and stars for the top repositories.\n",
        "\n",
        "    This method finds and extracts information about the top repositories for a given topic from the HTML page.\n",
        "    It returns lists of user names, repo names, repo links, and stars for further processing.\n",
        "    \"\"\"\n",
        "    user_name = []\n",
        "    repo_name = []\n",
        "    repo_link = []\n",
        "    stars = []\n",
        "    h_tag =topic_soup.find_all('h3',class_=\"f3 color-fg-muted text-normal lh-condensed\",limit=self.limit)\n",
        "    star_class =\"Counter js-social-count\"\n",
        "    stars_tags = topic_soup.find_all('span',class_=star_class,limit = self.limit)\n",
        "    for star_tag in stars_tags:\n",
        "      star = GithubRepos.covert_to_number(star_tag.get_text())\n",
        "      stars.append(int(star))\n",
        "    for topic in h_tag:\n",
        "      user = topic.get_text().split(\"/\")\n",
        "      user_name.append(user[0].strip())\n",
        "      repo_name.append(user[1].strip())\n",
        "      a_tag = topic.find('a',class_=\"Link text-bold wb-break-word\")\n",
        "      repo_link.append(baseUrl + a_tag['href'])\n",
        "    return user_name,repo_name,repo_link,stars\n",
        "\n",
        "  def to_csv(self,index,repo,df):\n",
        "    \"\"\"\n",
        "    Save repository information to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - index (int): Index of the topic in the DataFrame.\n",
        "    - repo (tuple): Tuple containing user names, repo names, repo links, and stars.\n",
        "    - df (pd.DataFrame): The main DataFrame containing topic information to get each topic name based on index\n",
        "\n",
        "    This method takes the extracted repository information, creates a DataFrame, and saves it to a CSV file\n",
        "    with the filename based on the topic name. The index parameter is used to identify the corresponding topic.\n",
        "    \"\"\"\n",
        "    d ={}\n",
        "    d[\"Username\"] = repo[0]\n",
        "    d[\"Repo Name\"] = repo[1]\n",
        "    d[\"Repo Link\"] = repo[2]\n",
        "    d[\"Star\"] = repo[3]\n",
        "    repo_df = pd.DataFrame(d)\n",
        "    file_name = df[\"Topics\"][index]+\".csv\"\n",
        "    repo_df.to_csv(file_name,index = False)\n",
        "\n",
        "  @staticmethod\n",
        "  def covert_to_number(star):\n",
        "    \"\"\"\n",
        "    Convert star count string to a numerical value.\n",
        "\n",
        "    Parameters:\n",
        "    - star (str): String representing the star count, e.g., \"60k\".\n",
        "\n",
        "    Returns:\n",
        "    int: The numerical representation of the star count.\n",
        "\n",
        "    This static method takes a star count string and converts it to a whole number.\n",
        "    It handles suffixes 'k' (thousand), 'm' (million), and 'b' (billion) to provide\n",
        "    the appropriate numerical representation.\n",
        "    \"\"\"\n",
        "    if star is None:\n",
        "      return 0\n",
        "    if star[-1].lower() ==\"k\":\n",
        "      return eval(star[:-1])*1000\n",
        "    if star[-1].lower() ==\"m\":\n",
        "      return eval(star[:-1])*1000000\n",
        "    if star[-1].lower() ==\"b\":\n",
        "      return eval(star[:-1])*1000000000\n",
        "    else:\n",
        "      return eval(star)\n"
      ],
      "metadata": {
        "id": "i87dxVxOgwRk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repoDriver():\n",
        "    limit = int(input(\"Enter the number of repo wanna get :\"))\n",
        "    githubTopics = GithubRepos(limit=10)\n",
        "    df = pd.read_csv(\"topics.csv\")\n",
        "\n",
        "    for index, link in enumerate(df[\"Links\"]):\n",
        "        try:\n",
        "            topic_soup = githubTopics.getSoup(link)\n",
        "            repo = githubTopics.getRepoInfo(topic_soup)\n",
        "            githubTopics.to_csv(index, repo, df)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {str(e)}\")\n",
        "if __name__ == \"__main__\":\n",
        "    repoDriver()"
      ],
      "metadata": {
        "id": "79il3bdV1ujN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49634f7-5f0f-4abd-aeb4-f097d09bf2be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the number of repo wanna get :15\n"
          ]
        }
      ]
    }
  ]
}